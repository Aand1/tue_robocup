== Team name == 

    Tech United Eindhoven

== Main ability == 

    Human assisted geometric, semantic mapping ...

== Test objective == 

	Show that the robot can interact with and manipulate newly learned dynamical(!) objects by simply giving them a name. 

== Abilities used == 

    [ ] Audio Processing
    [ ] Artificial Intelligence
    [?] Computer Vision
    [ ] Endurance / Strength
    [ ] Human Robot Interaction
    [ ] Learning
    [X] Manipulation
    [X] Navigation
    [X] Other: world modeling / (semantic, geometric) mapping

== Novelty and Scientific / League Contribution ==

    - Peform tasks without needing hard-coded environment knowledge besides the location of the walls.
    - Incremental learning / segmentation bootstrapping (the more you know, the easier it gets).
    - Automatically find the best pose to approach an object.
    - This experiment can instantly be repeated:
		* for every furniture layout of this arena.
		* for all objects (within detectable resolution)
 

== Test description == 

	AMIGO enters a new environment of which only the wall locations are known. A human operator uses a gui to send AMIGO around the room. All unknown objects can be labeled by the operator by simply clicking them and giving them a name. Of these objects a 3D model is generated and added to the world model. Even adding unknown objects on top of (now) known objects is no problem (e.g. a cup on a table).
	
	Now to demonstrate this ability the operator sends AMIGO to locations by voice commands (e.g. drive to table). Using a constrained-based planner the best location to approach this object is determined (note the green arrows on the screen).
	
	Note that moving the objects while doing any of this is not a problem!
	
	The demonstration ends with a request to amigo to grab one of the learned objects and put it in the trash bin. Again, moving these objects is no issue. Not the object or the thrash bin was known to AMIGO upon entering the room. 
